{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TensorFlow with GPU",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "BlmQIFSLZDdc"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Underwater GAN (UGAN) in Colab"
      ]
    },
    {
      "metadata": {
        "id": "rY4_9f8oSpm6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare data\n",
        "\n",
        "### Clone repo"
      ]
    },
    {
      "metadata": {
        "id": "E6ML5nyQSwia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# clone repo\n",
        "%cd /content/\n",
        "!if ! test -d Underwater-Color-Correction/; then git clone https://github.com/yoelrc88/Underwater-Color-Correction.git ; fi  \n",
        "%cd Underwater-Color-Correction/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujhXKVwSYkSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download dataset and checkpoint\n",
        "\n",
        "If something we need to download something from a google drive link (models, datasets)"
      ]
    },
    {
      "metadata": {
        "id": "P6IPWFZIXx_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#install gdrive dependencies\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authentication\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download file GDrive to Colab\n",
        "\n",
        "!echo \"Downloading underwater_imagenet.zip ...\"\n",
        "file_downloaded = drive.CreateFile({'id': '1LOM-2A1BSLaFjCY2EEK3DA2Lo37rNw-7'})\n",
        "file_downloaded.GetContentFile('underwater_imagenet.zip') # name of the file n GDrive\n",
        "\n",
        "!echo \"Downloading old_checkpoint.zip ...\"\n",
        "file_downloaded = drive.CreateFile({'id': '1VOpQPHOGi3bYX2C-0AWzSfaJVUBHlacU'})\n",
        "file_downloaded.GetContentFile('old_checkpoint.zip') # name of the file n GDrive\n",
        "\n",
        "# uncompress files\n",
        "!unzip ./underwater_imagenet.zip -d ./\n",
        "!unzip ./old_checkpoint.zip -d checkpoints/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f0tDAtSG97R4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "   Create pickle file\n",
        "\n",
        "'''\n",
        "\n",
        "import cPickle as pickle\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import argparse\n",
        "import ntpath\n",
        "import random\n",
        "import glob\n",
        "import time\n",
        "import sys\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# my imports\n",
        "sys.path.insert(0, 'ops/')\n",
        "sys.path.insert(0, 'nets/')\n",
        "from tf_ops import *\n",
        "import data_ops\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   parser = argparse.ArgumentParser()\n",
        "   parser.add_argument('--LEARNING_RATE', required=False,default=1e-4,type=float,help='Learning rate')\n",
        "   parser.add_argument('--LOSS_METHOD',   required=False,default='wgan',help='Loss function for GAN')\n",
        "   parser.add_argument('--BATCH_SIZE',    required=False,default=32,type=int,help='Batch size')\n",
        "   parser.add_argument('--L1_WEIGHT',     required=False,default=100.,type=float,help='Weight for L1 loss')\n",
        "   parser.add_argument('--IG_WEIGHT',     required=False,default=1.,type=float,help='Weight for image gradient loss')\n",
        "   parser.add_argument('--NETWORK',       required=False,default='pix2pix',type=str,help='Network to use')\n",
        "   parser.add_argument('--AUGMENT',       required=False,default=0,type=int,help='Augment data or not')\n",
        "   parser.add_argument('--EPOCHS',        required=False,default=100,type=int,help='Number of epochs for GAN')\n",
        "   parser.add_argument('--DATA',          required=False,default='underwater_imagenet',type=str,help='Dataset to use')\n",
        "   a = parser.parse_args()\n",
        "\n",
        "   LEARNING_RATE = float(a.LEARNING_RATE)\n",
        "   LOSS_METHOD   = a.LOSS_METHOD\n",
        "   BATCH_SIZE    = a.BATCH_SIZE\n",
        "   L1_WEIGHT     = float(a.L1_WEIGHT)\n",
        "   IG_WEIGHT     = float(a.IG_WEIGHT)\n",
        "   NETWORK       = a.NETWORK\n",
        "   AUGMENT       = a.AUGMENT\n",
        "   EPOCHS        = a.EPOCHS\n",
        "   DATA          = a.DATA\n",
        "   \n",
        "   EXPERIMENT_DIR  = 'checkpoints/LOSS_METHOD_'+LOSS_METHOD\\\n",
        "                     +'/NETWORK_'+NETWORK\\\n",
        "                     +'/L1_WEIGHT_'+str(L1_WEIGHT)\\\n",
        "                     +'/IG_WEIGHT_'+str(IG_WEIGHT)\\\n",
        "                     +'/AUGMENT_'+str(AUGMENT)\\\n",
        "                     +'/DATA_'+DATA+'/'\\\n",
        "\n",
        "   IMAGES_DIR      = EXPERIMENT_DIR+'images/'\n",
        "\n",
        "   print\n",
        "   print 'Creating',EXPERIMENT_DIR\n",
        "   try: os.makedirs(IMAGES_DIR)\n",
        "   except: pass\n",
        "   try: os.makedirs(TEST_IMAGES_DIR)\n",
        "   except: pass\n",
        "\n",
        "   # TODO add new things to pickle file - INCLUDING BATCH SIZE AND LEARNING RATE\n",
        "   # write all this info to a pickle file in the experiments directory\n",
        "   exp_info = dict()\n",
        "   exp_info['LEARNING_RATE'] = LEARNING_RATE\n",
        "   exp_info['LOSS_METHOD']   = LOSS_METHOD\n",
        "   exp_info['BATCH_SIZE']    = BATCH_SIZE\n",
        "   exp_info['L1_WEIGHT']     = L1_WEIGHT\n",
        "   exp_info['IG_WEIGHT']     = IG_WEIGHT\n",
        "   exp_info['NETWORK']       = NETWORK\n",
        "   exp_info['AUGMENT']       = AUGMENT\n",
        "   exp_info['EPOCHS']        = EPOCHS\n",
        "   exp_info['DATA']          = DATA\n",
        "   exp_pkl = open(EXPERIMENT_DIR+'info.pkl', 'wb')\n",
        "   data = pickle.dumps(exp_info)\n",
        "   exp_pkl.write(data)\n",
        "   exp_pkl.close()\n",
        "   \n",
        "   print\n",
        "   print 'LEARNING_RATE: ',LEARNING_RATE\n",
        "   print 'LOSS_METHOD:   ',LOSS_METHOD\n",
        "   print 'BATCH_SIZE:    ',BATCH_SIZE\n",
        "   print 'L1_WEIGHT:     ',L1_WEIGHT\n",
        "   print 'IG_WEIGHT:     ',IG_WEIGHT\n",
        "   print 'NETWORK:       ',NETWORK\n",
        "   print 'AUGMENT:       ',AUGMENT\n",
        "   print 'EPOCHS:        ',EPOCHS\n",
        "   print 'DATA:          ',DATA\n",
        "   print\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAUSBME-42X9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "aCSWhySv41Fi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd \"/content/Underwater-Color-Correction/\"\n",
        "!python2 train.py --DATA=underwater_imagenet --EPOCHS=100 --NETWORK=pix2pix --L1_WEIGHT=100 --BATCH_SIZE=32 --IG_WEIGHT=0.0 --LEARNING_RATE=1e-4 --LOSS_METHOD=wgan\n",
        "\n",
        "# %cd \"checkpoints/LOSS_METHOD_wgan/NETWORK_pix2pix/L1_WEIGHT_100.0/IG_WEIGHT_1.0/AUGMENT_0/DATA_underwater_imagenet/\"\n",
        "# !ls\n",
        "# !cat ./info.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2pSGqcBdSsUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run "
      ]
    },
    {
      "metadata": {
        "id": "VuOYcQphSxDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !python2 ./train.py --LEARNING_RATE 0.1\n",
        "# !mv \"underwater_imagenet/\" \"dataset\"\n",
        "# %cd ..\n",
        "# !ls -lSs --block-size=M\n",
        "# !printenv | grep PWD\n",
        "# !ls -ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TZmzCoM1A71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "   Evaluation file for only a single image.\n",
        "\n",
        "'''\n",
        "\n",
        "import cPickle as pickle\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import ntpath\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import time\n",
        "import glob\n",
        "import cPickle as pickle\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "sys.path.insert(0, 'ops/')\n",
        "sys.path.insert(0, 'nets/')\n",
        "from tf_ops import *\n",
        "\n",
        "import data_ops\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "   if len(sys.argv) < 3:\n",
        "      print 'You must provide an info.pkl file and an image'\n",
        "      exit()\n",
        "\n",
        "   pkl_file = open(sys.argv[1], 'rb')\n",
        "   a = pickle.load(pkl_file)\n",
        "   \n",
        "   LEARNING_RATE = a['LEARNING_RATE']\n",
        "   LOSS_METHOD   = a['LOSS_METHOD']\n",
        "   BATCH_SIZE    = a['BATCH_SIZE']\n",
        "   L1_WEIGHT     = a['L1_WEIGHT']\n",
        "   IG_WEIGHT     = a['IG_WEIGHT']\n",
        "   NETWORK       = a['NETWORK']\n",
        "   AUGMENT       = a['AUGMENT']\n",
        "   EPOCHS        = a['EPOCHS']\n",
        "   DATA          = a['DATA']\n",
        "\n",
        "#    EXPERIMENT_DIR  = 'checkpoints/LOSS_METHOD_'+LOSS_METHOD\\\n",
        "#                      +'/NETWORK_'+NETWORK\\\n",
        "#                      +'/L1_WEIGHT_'+str(L1_WEIGHT)\\\n",
        "#                      +'/IG_WEIGHT_'+str(IG_WEIGHT)\\\n",
        "#                      +'/AUGMENT_'+str(AUGMENT)\\\n",
        "#                      +'/DATA_'+DATA+'/'\\\n",
        "\n",
        "   EXPERIMENT_DIR  = 'experiment_dir/'\n",
        "\n",
        "\n",
        "   IMAGES_DIR     = EXPERIMENT_DIR+'dataset/test/'\n",
        "\n",
        "   test_image = sys.argv[2]\n",
        "\n",
        "   print\n",
        "   print 'LEARNING_RATE: ',LEARNING_RATE\n",
        "   print 'LOSS_METHOD:   ',LOSS_METHOD\n",
        "   print 'BATCH_SIZE:    ',BATCH_SIZE\n",
        "   print 'L1_WEIGHT:     ',L1_WEIGHT\n",
        "   print 'IG_WEIGHT:     ',IG_WEIGHT\n",
        "   print 'NETWORK:       ',NETWORK\n",
        "   print 'EPOCHS:        ',EPOCHS\n",
        "   print 'DATA:          ',DATA\n",
        "   print\n",
        "\n",
        "   if NETWORK == 'pix2pix': from pix2pix import *\n",
        "   if NETWORK == 'resnet':  from resnet import *\n",
        "\n",
        "   # global step that is saved with a model to keep track of how many steps/epochs\n",
        "   global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "   # underwater image\n",
        "   image_u = tf.placeholder(tf.float32, shape=(1, 256, 256, 3), name='image_u')\n",
        "\n",
        "   # generated corrected colors\n",
        "   layers    = netG_encoder(image_u)\n",
        "   gen_image = netG_decoder(layers)\n",
        "\n",
        "   saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "   init = tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())\n",
        "   sess = tf.Session()\n",
        "   sess.run(init)\n",
        "\n",
        "   ckpt = tf.train.get_checkpoint_state(EXPERIMENT_DIR)\n",
        "   if ckpt and ckpt.model_checkpoint_path:\n",
        "      print \"Restoring previous model...\"\n",
        "      try:\n",
        "         saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "         print \"Model restored\"\n",
        "      except:\n",
        "         print \"Could not restore model\"\n",
        "         pass\n",
        "   \n",
        "   step = int(sess.run(global_step))\n",
        "\n",
        "   img_name = ntpath.basename(test_image)\n",
        "   img_name = img_name.split('.')[0]\n",
        "\n",
        "   batch_images = np.empty((1, 256, 256, 3), dtype=np.float32)\n",
        "\n",
        "   #a_img = misc.imread(test_image).astype('float32')\n",
        "   a_img = cv2.imread(test_image)\n",
        "   a_img = cv2.cvtColor(a_img, cv2.COLOR_BGR2RGB)\n",
        "   a_img = a_img.astype('float32')\n",
        "   a_img = misc.imresize(a_img, (256, 256, 3))\n",
        "   a_img = data_ops.preprocess(a_img)\n",
        "   a_img = np.expand_dims(a_img, 0)\n",
        "   batch_images[0, ...] = a_img\n",
        "\n",
        "   gen_images = np.asarray(sess.run(gen_image, feed_dict={image_u:batch_images}))\n",
        "\n",
        "   misc.imsave('./'+img_name+'_real.png', batch_images[0])\n",
        "   misc.imsave('./'+img_name+'_gen.png', gen_images[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
